---
title: "Predicting Diagnosis of Breast Cancer Tumor"
author: "Jianghui Lin jl5172, Zixu Wang zw2541, Jack Yan xy2395"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # data manipulation
library(corrplot) # correlation plot
library(caret) # model training
library(AppliedPredictiveModeling) # plot themes
library(pROC) # contains roc function. generate ROC curves
library(MASS) # contains function lda and qda
library(rpart) # CART
library(rpart.plot)
library(plotmo)
library(glmnet)
library(e1071)
library(mlbench)
library(ISLR)
library(factoextra)
library(gridExtra)
library(RColorBrewer)
library(gplots)
library(gbm)
# parallel processing with caret
library(doParallel)
cluster <- makePSOCKcluster(8)
registerDoParallel(cluster)
```

```{r, include=FALSE}
set.seed(123123)
# dat = 
#   read_csv("./data.csv") %>% 
#   dplyr::select(-id, -X33)
# dim(dat)
# test_dat = sample_n(dat, 569/5)
# dim(test_dat)
# train_dat = anti_join(dat, test_dat)
# dim(train_dat)
# write_csv(train_dat, "train.csv")
# write_csv(test_dat, "test.csv")

train_df = 
  read_csv("./train.csv") %>% janitor::clean_names() %>% 
  mutate(diagnosis = as_factor(diagnosis))

test_df = 
  read_csv("./test.csv")  %>% janitor::clean_names() %>% 
  mutate(diagnosis = as_factor(diagnosis))
```

## Introduction

This dataset includes different features which were computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. The FNA describes the characteristics of the cell nuclei present in the image. Ten real-valued features are computed for each cell nucleus. The features include radius, texture, perimeter, area, compactness, concavity, concave points, symmetry, and fractal dimension. In this project, we aim to use the lab results to determine which features contribute to a better diagnosis result of breast mass (M = malignant, B = benign). In the dataset, we have 357 benign observations and 212 malignant observations, so the two classes are balanced.

## Dataset Partitioning
The original dataset contains 569 observations, among which 456 were randomly partitioned into the training set, and 113 into the test set. The training set was used to conduct model selection based on 10-fold cross-validation, and the test set was used to verify the model selection result. 

## Exploratory Data Analysis

#### 1. K Means Clustering

K-means clustering was used to partition the observations into 2 clusters. In this dataset, the optimal number of clusters is 2, determined by average silhouette method using the the `fviz_nbclust` function. Interestingly, the number of optimized clusters exactly equals the number of response categories ‘B’ and ‘M’. **(Figure 1)** shows the distribution of clusters on the first 2 principle components (PCs). Dots are labelled by their response categories, although response information was not used in clustering. The two clusters can be well separated by the first 2 PCs, and the two outcome classes are separated into each of the clusters. 

#### 2. Correlation plots

The correlation plot **(Figure 2)** shows that some strong correlation exist among a subset of covariates. Intuitively, the high correlation makes sense. For example, the correlation among perimeter, radius, and area are all related to the size of the tumor. However, we did not know which covariates are better, so we decided to keep all covariates at this point, and let the models decide which variables are more important.

#### 3. One-to-one relation between classes and covariates 

According to the feature plot **(Figure 3)** that shows the one-to-one relation between the response and covariates, The ‘M’ (malignant tutor) class generally has larger radius mean, texture mean, perimeter mean, area mean, compactness mean, smoothness mean and concavity mean compare to those of ‘B’ (benign) tumors. The two classes can be well separated on many individual covariates, such as radius, texture and concave points.

## Models

All the 30 covariates were included in all the models. The 30 predictors are the means, standard errors and worst values of the 10 characteristics measured on the cell nuclei. Ordinary and regularized Logistic regression, Linear and quadratic discriminant analysis, Naive Bayes, KNN, support vector machines, classification trees, random forests, and ensemble methods were used to fit the data. The area under the ROC curve (AUC) were calculated from repeated cross-validation and used to compare models. For models that require tuning, repeated cross-validation was used to decide the optimal hyperparameter(s) that corresponds to the largest AUC.

#### 1. Logistic regression and regularized logistic regression (glmnet)

Both logistic regression and regularized logistic regression models (glmnet) were built to fit the 30 predictors on the response. Data were centered and scaled before fitting the glmnet model. Cross-validation showed that the logistic regression has AUC 0.9544, sensitivity 0.9821 and specificity 0.8856. The best parameter of glmnet is alpha 0.2 and lambda 0.014, with AUC 0.9961, which is better than the logistic regression model.
 
#### 2. LDA 

The assumption of LDA is that all the variables follow a multivariate normal distribution. Therefore, each covariate has its own mean and shares a common variance-covariance matrix. All 30 predictors were included in this model. By fitting the model with the cross-validation method, the optimal LDA model has AUC 0.9917, sensitivity 0.8958 and specificity 0.9930.

#### 3. QDA

QDA assumes that all the variables are following a Gaussian distribution. Therefore, different from LDA, each covariate has its specific mean and different variance-covariance matrix. All 30 predictors were included in this QDA model and the AUC is 0.9910, sensitivity is 0.9718, specificity is 0.9425.

#### 4. Naive Bayes

Naive Bayes assumes that the features are conditionally independent given the class instead of modeling their full conditional distribution given the class, and it is an approximation to the Bayes classifier. All 30 predictors were included in the model and the best AUC is 0.9873.

#### 5. KNN

All 30 predictors were included in this KNN model, and the best tuning parameter is 33. This model has AUC 0.9925, sensitivity 0.8784 and specificity 0.9894.

#### 6. Support Vector Machine (linear and radial kernel)

Support vector machines with both linear kernel and radial kernel were fit on the data. The optimal linear kernel has AUC 0.994 and the optimal radial kernel has AUC 0.996. The radial kernel is slightly better.

#### 7. Classification Tree and random forests

A classification tree was built on the 30 predictors. Cp was tuned using cross-validation. The best cp turned out to be 0.0062, corresponding to 5 terminal nodes and AUC = 0.9412. In the random forest model, both interaction depth (mtry) and minimal node size were tuned by cross-validation. The best model random forest model had interaction depth 1 and minimal node size 1, with AUC 0.9931, which is much better than that of a single tree.

#### 8. Boosting (AdaBoosting and binomial loss function)

Binomial loss boosting and AdaBoosting were tuned on 3 hyperparameters: number of trees, interaction depth, and shrinkage. Minimal node size was set to be 1. The best binomial loss boosting model and the best AdaBoosting model had AUC 0.9926 and 0.9930, respectively. Their AUC are comparable.


## Model Comparison and selection

The mean cross-validation AUC, sensitivity and specificity are summarized in **Table 1**. SVM (radial kernal) has the highest sensitivity (0.965) and glmnet has the highest specificity (0.997). The AUC of all the models are higher than 0.93. 

Model selection was based on cross-validation AUC. The support vector machine (radial kernal) and glmnet have the largest cross-validation AUC (0.9957). Since the glmnet model can be easier interpreted, we decided to use it as our final model. 

## Final Model Interpretaion
The final model is a mixture of ridge and lasso (alpha = 0.2, lambda = 0.014). Non-zero coefficients of the final regularized logistic regression (glmnet) are summarized in **Table 2**. The 25 non-zero coefficients are arranged in descending order based on their absolute values. SE of radius, worst value of radius, worst value of texture, worst value of concave points, and worst value of perimeter are the top 5 predictors with largest absolute coefficients. Since all the predictors are centered and scaled before fitting the model, the coefficients are on the same scale as well. Therefore, their absolute values can reflect the importance of the predictors. 

The train data ROC curve of the final model is shown in **Figure XXX**. The AUC for the train data is 0.997, indicating a good fit. Not only does our final model have best prediction performance, but is also easier to interpret as compared to other complex models. The coefficients can be simply interpreted as the log odds ratio of malignant tumor for one-unit change in the corresponding scaled covariate, keeping other covariates constant. For example, the coefficient for 

\newpage

## Conclusion
Prompt: 
What were your findings? 
We find that the regularized logistic regression including 25 covariates has the 
Are they what you expect? 

What insights into the data can you make?


## APPENDIX

```{r echo=FALSE, eval=FALSE}
dat1 <- read.csv("data.csv", row.names=NULL)
class1 = paste(dat1[,2], 1:length(class), sep="-")
dat1 <- dat1[,-2]
dat1 <- dat1[,-32]
dat1 <- dat1[,-1]
rownames(dat1) <- class1
dim(dat1)
dat1 <- dat1[,1:10]
dat1 <- scale(dat1)
fviz_nbclust(dat1,FUNcluster = kmeans,method = "silhouette")
set.seed(123123)
km <- kmeans(dat1, centers = 2, nstart = 20)
km_vis <- fviz_cluster(list(data = dat1, cluster = km$cluster),
                      ellipse.type = "convex",
                      geom = c("point","text"),
                      labelsize = 5, palette = "Dark2")
km_vis
saveRDS(km_vis, "km_vis.rds")

clst_plot = fviz_nbclust(dat1,FUNcluster = kmeans,method = "silhouette")
saveRDS(clst_plot, "nclust.rds")
```

```{r echo=FALSE}
km_vis = readRDS("km_vis.rds")
km_vis
```

\center __Figure 1__ 2-means clustering on the first two principle components \center

```{r echo=FALSE}
x <- model.matrix(diagnosis~., train_df)[,-1]
y <- train_df$diagnosis
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5) 
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1) 
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2) 
trellis.par.set(theme1)

par(cex = 0.7)
corrplot(cor(x), tl.srt = 45, order = 'hclust', type = 'upper')
```

\center __Figure 2__ Correlation plots \center

```{r echo=FALSE, warning=F}
transparentTheme(trans = .4)
featurePlot(x = train_df[,2:31], 
            y = train_df$diagnosis,
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")),
            plot = "density", pch = "|",
            auto.key = list(columns = 2))
```

\center __Figure 3__ One-to-one relation between response classes and covariates \center

```{r echo=FALSE}
ctrl <- trainControl(method = "repeatedcv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
set.seed(123123)
model.glm <- train(x=train_df[,2:31],
                  y=train_df$diagnosis,
                  method ="glm",
                  metric ="ROC",
                  trControl =ctrl)

model.glmn = readRDS("glmn_fit.rds")

model.lda <-train(x=train_df[,2:31],
                  y=train_df$diagnosis,
                  method ="lda",
                  metric ="ROC",
                  trControl =ctrl)

model.qda = readRDS("qda_fit.RDS")

model.knn = readRDS("knn_fit.RDS")

model.nb = readRDS("nb_fit.rds")

svml.fit = readRDS("svml_fit.rds")

svmr.fit = readRDS("svmr_fit.rds")

rpart.fit = readRDS('rpart_fit.rds')

rf.fit = readRDS('rf_fit.rds')

gbmB.fit = readRDS('gbmB_fit.rds')

gbmA.fit = readRDS('gbmA_fit.rds')

resamp <- resamples(list(glm = model.glm,
                         glmn = model.glmn,
                         lda = model.lda,
                         qda = model.qda,
                         knn = model.knn,
                         bayes = model.nb,
                         svmr = svmr.fit,
                         svml = svml.fit,
                         rpart = rpart.fit, 
                         rf = rf.fit,
                         gbmB = gbmB.fit,
                         gbmA = gbmA.fit))

bwplot(resamp)
```

\center __Figure __ Model comparision on AUC, sensitivity and specificity \center

\newpage

```{r include=FALSE}
summary(resamp)

roc = paste(resamp$models, "ROC", sep="~")
roc_mean = resamp$values %>% as_tibble %>% dplyr::select(roc) %>% map_dbl(., ~mean(.x))
spec = paste(resamp$models, "Spec", sep="~")
spec_mean = resamp$values %>% as_tibble %>% dplyr::select(spec) %>% as.data.frame() %>% 
map_dbl(., ~mean(.x))

sens = paste(resamp$models, "Sens", sep="~")
sens_mean = resamp$values %>% as_tibble %>% dplyr::select(sens) %>% as.data.frame() %>% 
map_dbl(., ~mean(.x))

tbl = 
  rbind(roc_mean, spec_mean, sens_mean) %>% 
  t() %>% as.tibble() %>% 
  mutate(model = resamp$models) %>% 
  dplyr::select(model, AUC=roc_mean,Sensitivity=sens_mean,Specificity=spec_mean) %>% 
  arrange(desc(AUC))
```

\textbf{Table 1: Mean of AUC, sensitivity and specificity of different models}
```{r echo=FALSE}
as.tibble(tbl) %>% knitr::kable(digits=4)
```

\textbf{Table 2: Non-zero Coefficients of the Final Model }
```{r}
# model.glmn$bestTune
formula3 <- 
  diagnosis ~ radius_mean + texture_mean + perimeter_mean + area_mean + 
  smoothness_mean + compactness_mean + concavity_mean + concave_points_mean +
  symmetry_mean + fractal_dimension_mean + radius_worst + texture_worst +
  perimeter_worst + area_worst + smoothness_worst + compactness_worst + concavity_worst +
  concave_points_worst + symmetry_worst + fractal_dimension_worst + radius_se + texture_se +
  perimeter_se + area_se + smoothness_se + compactness_se + concavity_se + concave_points_se +
  symmetry_se + fractal_dimension_se

train_scaled = 
  scale(train_df[,-1]) %>% as.tibble() %>% 
  mutate(diagnosis = train_df$diagnosis)
x <- model.matrix(formula3, train_scaled)[,-1]

y <- train_df$diagnosis %>% as.character() 
y = if_else(y == "M", 1, 0)
glmn_best <- glmnet(x, y, family = "binomial",
                       alpha = 0.2, lambda = c(0.01407778,0.01407779))
coef = coef(glmn_best) %>% as.matrix %>% as.data.frame()
coef$term = rownames(coef)
# coeffients of the final model
coef = 
  coef %>% as.tibble() %>% 
  dplyr::select(term, s0) %>% 
  rename(coef = s0) %>% 
  filter(coef != 0, term != "(Intercept)") %>% 
  arrange(desc(abs(coef)))
coef %>% knitr::kable(digits=4)
```

```{r}
clst_plot = readRDS("nclust.rds")
clst_plot
```

```{r}
model.glmn = readRDS("glmn_fit.rds")
test.pred.prob  <- predict(model.glmn, type = "prob")[,1]
test.pred <- rep("B", length(test.pred.prob))
test.pred[test.pred.prob > 0.5] <- "M"
roc.glm <- roc(train_df$diagnosis, test.pred.prob)
plot.roc(roc.glm, legacy.axes = TRUE, print.auc = TRUE)
plot.roc(smooth(roc.glm), col = 4, add = TRUE)
```

