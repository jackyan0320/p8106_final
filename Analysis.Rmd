---
title: "Analysis"
author: "Jack Yan"
date: "5/17/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # data manipulation
library(corrplot) # correlation plot
library(caret) # model training
library(AppliedPredictiveModeling) # plot themes
library(pROC) # contains roc function. generate ROC curves
library(MASS) # contains function lda and qda
library(rpart) # CART
library(rpart.plot)

# parallel processing with caret
library(doParallel)
cluster <- makePSOCKcluster(8)
registerDoParallel(cluster)
```

# Create a test dataset
```{r, eval=FALSE}
set.seed(123123)
dat = 
  read_csv("./data.csv") %>% 
  select(-id, -X33) 
  dim(dat)
test_dat = sample_n(dat, 569/5)
train_dat = anti_join(dat, test_dat)
write_csv(train_dat, "train.csv")
write_csv(test_dat, "test.csv")
```


# Load train and test data
```{r}
train_df = 
  read_csv("./train.csv") %>% janitor::clean_names() %>% 
  mutate(diagnosis = as_factor(diagnosis))

test_df = 
  read_csv("./test.csv")  %>% janitor::clean_names() %>% 
  mutate(diagnosis = as_factor(diagnosis))
```


# Exploratory data analysis

### Clustering (Unsupervised learning)
```{r}

```

### Correlation plots
```{r, cache=TRUE}
x <- model.matrix(diagnosis~., train_df)[,-1]
y <- train_df$diagnosis
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5) 
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1) 
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2) 
trellis.par.set(theme1)

par(cex = 0.7)
corrplot(cor(x), tl.srt = 45, order = 'hclust', type = 'upper')
```


### One-to-one relation between classes and covariates
```{r, cache=TRUE}
# Distribution of response classes with regard to each variable
transparentTheme(trans = .4)
featurePlot(x = train_df[,2:31], 
            y = train_df$diagnosis,
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")),
            plot = "density", pch = "|",
            auto.key = list(columns = 2))
```


# Model building, assessing performance, and variable importance
```{r, echo=F}
# model formula
# only include mean
formula1 <- 
  diagnosis ~ radius_mean + texture_mean + perimeter_mean + area_mean + 
  smoothness_mean + compactness_mean + concavity_mean + concave_points_mean +
  symmetry_mean + fractal_dimension_mean
# include mean and worst measurements
formula2 <- 
  diagnosis ~ radius_mean + texture_mean + perimeter_mean + area_mean + 
  smoothness_mean + compactness_mean + concavity_mean + concave_points_mean +
  symmetry_mean + fractal_dimension_mean + radius_worst + texture_worst +
  perimeter_worst + area_worst + smoothness_worst + compactness_worst + concavity_worst +
  concave_points_worst + symmetry_worst + fractal_dimension_worst
# include all covariates
formula3 <- 
  diagnosis ~ radius_mean + texture_mean + perimeter_mean + area_mean + 
  smoothness_mean + compactness_mean + concavity_mean + concave_points_mean +
  symmetry_mean + fractal_dimension_mean + radius_worst + texture_worst +
  perimeter_worst + area_worst + smoothness_worst + compactness_worst + concavity_worst +
  concave_points_worst + symmetry_worst + fractal_dimension_worst + radius_se + texture_se +
  perimeter_se + area_se + smoothness_se + compactness_se + concavity_se + concave_points_se +
  symmetry_se + fractal_dimension_se
```


## Linear methods
### Logistic Regression
```{r, warning=FALSE}
ctrl <- trainControl(method = "repeatedcv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
set.seed(123123)
model.glm <-train(x=train_df[,2:31],
                  y=train_df$diagnosis,
                  method ="glm",
                  metric ="ROC",
                  trControl =ctrl)

glm.fit <- glm(diagnosis~., data=train_df, family="binomial")
# summary(glm.fit)

contrasts(train_df$diagnosis)

glm.pred.prob = predict(glm.fit, type = "response")
glm.pred = rep("B", length(glm.pred.prob))
glm.pred[glm.pred.prob > 0.5] = "M"
confusionMatrix(data = as.factor(glm.pred),
                reference = train_df$diagnosis,
                positive = "M")

glm.pred.prob.test = predict(glm.fit, type = "response", newdata = test_df)
roc.glm.test = roc(test_df$diagnosis, glm.pred.prob.test)
plot(roc.glm.test, legacy.axes = TRUE, print.auc = TRUE)
```


### Regularized Logistic Regression
```{r, eval=F}
set.seed(123123)
glmnGrid <- expand.grid(.alpha = seq(0,1,length =6),
                       .lambda = exp(seq(-6,-2,length =20)))
model.glmn <- train(x=train_df[,2:31],
                   y=train_df$diagnosis,
                   method ="glmnet",
                   tuneGrid =glmnGrid,
                   metric ="ROC",
                   trControl =ctrl)
saveRDS(model.glmn, "glmn_fit.rds")
```

```{r}
model.glmn = readRDS("glmn_fit.rds")
ggplot(model.glmn,xTrans = function(x)log(x), highlight = TRUE)
max(model.glmn$result$ROC)
model.glmn$bestTune
```

### LDA **Need to use caret**
```{r}
lda.fit <- lda(diagnosis~., data = train_df)
plot(lda.fit)

lda.pred.test = predict(lda.fit, newdata = test_df)
roc.lda = roc(test_df$diagnosis, lda.pred.test$posterior[,2],
levels = c("B", "M"))
plot(roc.lda, legacy.axes = TRUE, print.auc = TRUE)

# caret
set.seed(123123)
model.lda <-train(x=train_df[,2:31],
                  y=train_df$diagnosis,
                  method ="lda",
                  metric ="ROC",
                  trControl =ctrl)
```

## Non-linear methods

### QDA
```{r, eval=F}
set.seed(123123)
qda.fit <- qda(diagnosis~.,
               data = train_df)

model.qda <- train(x = train_df[,-1],
                   y = train_df$diagnosis,
                   method = "qda",
                   metric = "ROC",
                   trControl = ctrl)
saveRDS(model.qda, "qda_fit.RDS")
```

```{r}
model.qda = readRDS("qda_fit.RDS")
model.qda$results$ROC
```

### KNN
```{r}
set.seed(123123)
model.knn <- train(x = train_df[,-1],
                   y = train_df$diagnosis,
                   method = "knn",
                   preProcess = c("center", "scale"), 
                   tuneGrid = data.frame(k = seq(1,50,by=1)),  
                   trControl = ctrl)
```


### Bayes
```{r,warning=F, eval=F}
set.seed(123123)
nbGrid <- expand.grid(usekernel = c(FALSE,TRUE),
                      fL = 1, 
                      adjust = seq(0,5,by = 1))
model.nb <- train(x = train_df[,-1],
                  y = train_df$diagnosis,
                  method = "nb",
                  tuneGrid = nbGrid,
                  metric = "ROC",
                  trControl = ctrl)
saveRDS(model.nb, "nb_fit.rds")
```

```{r}
model.nb = readRDS("nb_fit.rds")
plot(model.nb)
```




## Support Vector Machine
### Linear Kernel
```{r}
##Linear Kernel
set.seed(123123)
svml.fit <- train(diagnosis~., 
                  data = train_df, 
                  method = "svmLinear2",
                  preProcess = c("center", "scale"),
                  tuneGrid = data.frame(cost = exp(seq(-3,-1,len=50))),
                  trControl = ctrl)
saveRDS(svml.fit, "svml_fit.rds")
```

```{r }
svml.fit = readRDS("svml_fit.rds")
svml.fit$bestTune
max(svml.fit$result$ROC)
ggplot(svml.fit, highlight = TRUE)
```

Linear Kernel Training Error Rate
```{r}
pred.svml.train <- predict(svml.fit)
mean(pred.svml.train != train_df$diagnosis)
```
**The training error rate for linear kernel is 0.0088.**

Linear Kernel Test Error Rate
```{r}
pred.svml.test <- predict(svml.fit, newdata = test_df)
mean(pred.svml.test != test_df$diagnosis)
```
**The testing error rate for linear kernel is 0.0265.**


### Radial Kernel  

Fit a support vector machine with a radial kernel to the training data. What are the training and test error rates?
```{r, eval=F}
svmr.grid <- expand.grid(C = seq(1,4,len=10),
                         sigma = exp(seq(-5,-2,len=10)))
set.seed(123123)
svmr.fit <- train(diagnosis~., 
                  data = train_df, 
                  method = "svmRadial",
                  preProcess = c("center", "scale"),
                  tuneGrid = svmr.grid,
                  trControl = ctrl)
saveRDS(svmr.fit, "svmr_fit.rds")
```


##### Radial Kernel Training Error Rate

```{r}
svmr.fit = readRDS("svmr_fit.rds")
svmr.fit$bestTune
ggplot(svmr.fit, highlight = TRUE)
pred.svmr.train <- predict(svmr.fit)
mean(pred.svmr.train != train_df$diagnosis)
```

**The training error rate for radial kernel is 0.011.**
### Raidal Kernel Test Error Rate
```{r}
pred.svmr.test <- predict(svmr.fit, newdata = test_df)
mean(pred.svmr.test != test_df$diagnosis)
```
**The testing error rati for radial kernel is 0.0177.**


##### Which approach seems to give a better result on this data?
```{r}
resamp <- resamples(list(svmr = svmr.fit, svml = svml.fit))
bwplot(resamp)
```
**Acccording to the plot, we can tell that radial kernal has higher accuracy and Kappa compared to linear kernal.**

```{r}
confusionMatrix(data = pred.svml.test, 
                reference = test_df$diagnosis)

confusionMatrix(data = pred.svmr.test, 
                reference = test_df$diagnosis)

```
**According to the confusion matrix,the radial kernel has higher sensitivity, specificity, PPV, NPV and Kappa compared to those of the linear kernel.**
**In conclusion, the radial kernel seems to give a better result on the data.**


## Classification Trees and Ensemble methods
### Classification Tree
```{r, eval=F}
ctrl <- trainControl(method = "repeatedcv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
# variable.names(train_df)

set.seed(123123)
rpart.fit <- train(formula3, train_df, 
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-20,-2, len = 50))),
                   trControl = ctrl,
                   metric = "ROC")
saveRDS(rpart.fit, 'rpart_fit.rds')
```


```{r }
rpart.fit = readRDS('rpart_fit.rds')
rpart.fit$bestTune
max(rpart.fit$result$ROC)
# Model tuning
ggplot(rpart.fit, highlight = TRUE)
# Plot of the final model
rpart.plot(rpart.fit$finalModel)
tree_pruned = prune(tree_rpart, cp = one_se)
rpart.plot(tree_pruned)
```

The plot of the final model is shown above.

```{r}
test_df$probM = predict(rpart.fit$finalModel, newdata = test_df, type = "prob")[,1]
test_df$pred = if_else(test_df$probM > 0.5, 'M', 'B')

# Classification error rate
1 - mean(test_df$pred == test_df$diagnosis)
max(rpart.fit$result[,"ROC"])
```



### Random forests
```{r, eval = F}
rf.grid <- expand.grid(mtry = seq(1,10, by=1),
                       splitrule = "gini",
                       min.node.size = seq(1,31, by=2))
set.seed(123123)
rf.fit <- train(formula3, train_df,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl)
```

```{r, echo=F}
# saveRDS(rf.fit, 'rf_fit.rds')
rf.fit = readRDS('rf_fit.rds')
```

```{r }
rf.fit$bestTune
max(rf.fit$results[,"ROC"])
ggplot(rf.fit, highlight = TRUE)

rf.pred <- predict(rf.fit, newdata = test_df, type = "prob")[,1]
pred_rf = if_else(test_df$probM3 > 0.5, 'M', 'B')
# Classification error rate
1 - mean(pred_rf == test_df$diagnosis)
```


### boosting (Adaboosting and binomial loss function)
#### Binomial loss
```{r, eval=F}
gbmB.grid <- expand.grid(n.trees = c(1000,2000,3000,4000),
                        interaction.depth = 1:8,
                        shrinkage = c(0.004,0.006,0.008,0.010),
                        n.minobsinnode = 1)
set.seed(123123)
# Binomial loss function
gbmB.fit <- train(formula3, train_df, 
                 tuneGrid = gbmB.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "bernoulli",
                 metric = "ROC",
                 verbose = FALSE)

gbmB.fit$bestTune
max(gbmB.fit$results[,"ROC"])
```

```{r, echo=F,include=F}
# saveRDS(gbmB.fit, 'gbmB_fit.rds')
gbmB.fit = readRDS('gbmB_fit.rds')
```

```{r}
ggplot(gbmB.fit, highlight = TRUE)
gbmB.pred <- predict(gbmB.fit, newdata = test_df, type = "prob")[,1]
class_gbmB = if_else(test_df$probM3 > 0.5, 'M', 'B')
# Classification error rate
1 - mean(class_gbmB == test_df$diagnosis)
```


#### AdaBoost
```{r, eval=F}
gbmA.grid <- expand.grid(n.trees = c(6000,8000,10000,12000,14000),
                        interaction.depth = seq(1, 10, by=2),
                        shrinkage = seq(0.004, 0.010, by=0.002),
                        n.minobsinnode = 1)
set.seed(123123)
# Adaboost loss function
gbmA.fit <- train(formula3, train_df,
                 tuneGrid = gbmA.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "adaboost",
                 metric = "ROC",
                 verbose = FALSE)
```

```{r, echo=F}
# saveRDS(gbmA.fit, 'gbmA_fit.rds')
gbmA.fit = readRDS('gbmA_fit.rds')
```

```{r}
gbmA.fit$bestTune
max(gbmA.fit$results[,"ROC"])
ggplot(gbmA.fit, highlight = TRUE)
gbmA.pred <- predict(gbmA.fit, newdata = test_df, type = "prob")[,1]
class_gbmA = if_else(gbmA.pred > 0.5, 'CH', 'MM')
# Classification error rate
1 - mean(class_gbmA == test_df$Purchase)
```

# Model comparison
```{r}
resamp <- resamples(list(gbmB = gbmB.fit,
                         gbmA = gbmA.fit))

resamp <- resamples(list(rpart = rpart.fit, rf = rf.fit))



resamp <- resamples(list(glm = model.glm,
                         glmn = model.glmn,
                         lda = model.lda,
                         qda = model.qda,
                         knn = model.knn,
                         bayes = model.nb,
                         svmr = svmr.fit,
                         svml = svml.fit))

bwplot(resamp)
```

# Variable importance, Visualization, and interpretation (L7_2.rmd): PDP, ICE, etc.


# Final model interpretation







