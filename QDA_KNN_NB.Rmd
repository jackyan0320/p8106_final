---
title: "QDA_KNN_NB"
author: "Jianghui Lin"
date: "5/15/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # data manipulation
library(corrplot) # correlation plot
library(caret) # model training
library(MASS)
library(caret)
library(glmnet)
library(e1071)
library(mlbench)
library(pROC)
library(AppliedPredictiveModeling)
library(ISLR)
library(factoextra)
library(gridExtra)
library(RColorBrewer)
library(gplots)
```

```{r}
test_df<-read.csv("test.csv")
train_df<-read.csv("train.csv")
```

#QDA
```{r}
set.seed(1)
qda.fit <- qda(diagnosis~.,
               data = train_df)
ctrl <- trainControl(method = "repeatedcv",
                     repeats = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE) 
model.qda <- train(x = train_df[,-1],
                   y = train_df$diagnosis,
                   method = "qda",
                   metric = "ROC",
                   trControl = ctrl)

qda.pred <- predict(qda.fit, newdata = test_df)
head(qda.pred$posterior)

roc.qda <- roc(test_df$diagnosis, qda.pred$posterior[,2], 
               levels = c("B","M"))

plot(roc.qda, legacy.axes = TRUE, print.auc = TRUE,main="QDA ROC Plot")
```
**AUC Value for QDA is 0.990 as shown above.**

#KNN
```{r}
set.seed(1)
model.knn <- train(x = train_df[,-1],
                   y = train_df$diagnosis,
                   method = "knn",
                   preProcess = c("center", "scale"), 
                   tuneGrid = data.frame(k = seq(1,50,by=1)),  
                   trControl = ctrl)
model.knn$bestTune
ggplot(model.knn)
pred_knn = predict.train(model.knn, newdata = test_df, type = 'prob')
roc_knn <- roc(test_df$diagnosis, pred_knn[,2],
               levels = c("B", "M"))
plot.roc(roc_knn, legacy.axes = TRUE, print.auc = TRUE,main="KNN ROC Plot")
```
**AUC Value for KNN is 0.989 as shown above.**

#Bayes
```{r,warning=F}
set.seed(1)

nbGrid <- expand.grid(usekernel = c(FALSE,TRUE),
                      fL = 1, 
                      adjust = seq(0,5,by = 1))

model.nb <- train(x = train_df[,-1],
                  y = train_df$diagnosis,
                  method = "nb",
                  tuneGrid = nbGrid,
                  metric = "ROC",
                  trControl = ctrl)

plot(model.nb)
```

#Compare QDA, NB and KNN   
```{r}
res <- resamples(list(QDA=model.qda,NB = model.nb, KNN = model.knn))
summary(res)
```

Now let's look at the test set performance.
```{r, warning=FALSE}
library(stats)
pred_knn = predict.train(model.knn, newdata = test_df, type = 'prob')[,2]
pred_qda = predict.train(model.qda, newdata = test_df, type = 'prob')[,2]
pred_nb = predict.train(model.nb, newdata = test_df, type = 'prob')[,2]

roc.nb <- roc(test_df$diagnosis, pred_nb)
roc.qda <- roc(test_df$diagnosis, pred_qda)
roc.knn <- roc(test_df$diagnosis, pred_knn)

auc <- c(roc.qda$auc[1], roc.nb$auc[1], roc.knn$auc[1])


plot(roc.qda, col = 1,legacy.axes=TRUE)
plot(roc.nb, col = 2,add=TRUE)
plot(roc.knn, col = 3,add=TRUE)
modelNames <- c("qda","nb","knn")
legend("bottomright", legend = paste0(modelNames, ": ", round(auc,3)),
       col = 1:6, lwd = 2)
```





##Linear Kernel
```{r}
##Linear Kernel
ctrl <- trainControl(method = "cv")
set.seed(1)
svml.fit <- train(diagnosis~., 
                  data = train_df, 
                  method = "svmLinear2",
                  preProcess = c("center", "scale"),
                  tuneGrid = data.frame(cost = exp(seq(-2,4,len=50))),
                  trControl = ctrl)

ggplot(svml.fit, highlight = TRUE)
```

Linear Kernel Training Error Rate
```{r}
pred.svml.train <- predict(svml.fit)
mean(pred.svml.train != train_df$diagnosis)
```
**The training error rate for linear kernel is 0.0088.**

Linear Kernel Test Error Rate
```{r}

pred.svml.test <- predict(svml.fit, newdata = test_df)
mean(pred.svml.test != test_df$diagnosis)
```
**The testing error rate for linear kernel is 0.0265.**


**b)Radial Kernel**  Fit a support vector machine with a radial kernel to the training data.  What are thetraining and test error rates?
```{r}
svmr.grid <- expand.grid(C = exp(seq(-2,6,len=10)),
                         sigma = exp(seq(-8,-3,len=5)))
set.seed(1)
svmr.fit <- train(diagnosis~., 
                  data = train_df, 
                  method = "svmRadial",
                  preProcess = c("center", "scale"),
                  tuneGrid = svmr.grid,
                  trControl = ctrl)

ggplot(svmr.fit, highlight = TRUE)
```

## Radial Kernel Training Error Rate
```{r}
pred.svmr.train <- predict(svmr.fit)
mean(pred.svmr.train != train_df$diagnosis)
```
**The training error rate for radial kernel is 0.011.**

## Raidal Kernel Test Error Rate
```{r}
pred.svmr.test <- predict(svmr.fit, newdata = test_df)
mean(pred.svmr.test != test_df$diagnosis)
```
**The testing error rati for radial kernel is 0.0177.**


##(c) Which approach seems to give a better result on this data?
```{r}
resamp <- resamples(list(svmr = svmr.fit, svml = svml.fit))
bwplot(resamp)
```
**Acccording to the plot, we can tell that radial kernal has higher accuracy and Kappa compared to linear kernal.**

```{r}
confusionMatrix(data = pred.svml.test, 
                reference = test_df$diagnosis)

confusionMatrix(data = pred.svmr.test, 
                reference = test_df$diagnosis)

```
**According to the confusion matrix,the radial kernel has higher sensitivity, specificity, PPV, NPV and Kappa compared to those of the linear kernel.**
**In conclusion, the radial kernel seems to give a better result on the data.**


```{r}
fviz_nbclust(dat1,FUNcluster = kmeans,method = "silhouette")
```

```{r}
set.seed(1)
km <-kmeans(dat1, centers = 2, nstart = 20)
```

In K-means clustering, we seek to partition the observations into apre-specified number of clusters
ISimple and fast:  easy to understand and to implementITerminates at a local optimum - the global optimum is hard to findIThe algorithm is only applicable if the mean is definedIThe user needs to specifyKISensitive to outliersISometimes sensitive to initial seedsIThe algorithm is not suitable for discovering clusters that are nothyper-ellipsoids
```{r}
dat1<-read.csv("data.csv")
dat1<-dat1[,-2]
dat1<-dat1[,-32]
rownames(dat1) <- dat1[,1]
dat1<-dat1[,-1]
dat1<-dat1[,1:10]
dat1 <-scale(dat1)
fviz_nbclust(dat1,FUNcluster = kmeans,method = "silhouette")
set.seed(1)
km <-kmeans(dat1, centers = 2, nstart = 20)
km_vis <-fviz_cluster(list(data = dat1, cluster = km$cluster),
                      ellipse.type = "convex",
                      geom =c("point","text"),
                      labelsize = 5,palette = "Dark2")
km_vis
```



