---
title: "analysis plan"
author: "Jack Yan"
date: "5/10/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # data manipulation
library(corrplot) # correlation plot
library(caret) # model training
library(AppliedPredictiveModeling) # plot themes
library(pROC) # contains roc function. generate ROC curves
library(MASS) # contains function lda and qda
library(rpart) # CART
library(rpart.plot)

# parallel processing with caret
library(doParallel)
cluster <- makePSOCKcluster(8)
registerDoParallel(cluster)
```

```{r, warning=F, eval=F, include=F}
dat = 
  read_csv("./data.csv") %>% 
  select(-id, -X33) 
  dim(dat)
```

# Create a test dataset
```{r, eval=FALSE}
set.seed(123123)
test_dat = sample_n(dat, 569/5)
train_dat = anti_join(dat, test_dat)
write_csv(train_dat, "train.csv")
write_csv(test_dat, "test.csv")
```

# Load train and test data
```{r}
train_df = 
  read_csv("./train.csv") %>% janitor::clean_names() %>% 
  mutate(diagnosis = as_factor(diagnosis))

test_df = 
  read_csv("./test.csv")  %>% janitor::clean_names() %>% 
  mutate(diagnosis = as_factor(diagnosis))
```

# Exploratory data analysis

### Clustering (Unsupervised learning)
```{r}

```

### Correlation plots
```{r, cache=TRUE}
x <- model.matrix(diagnosis~., train_df)[,-1]
y <- train_df$diagnosis
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5) 
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1) 
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2) 
trellis.par.set(theme1)

par(cex = 0.7)
corrplot(cor(x), tl.srt = 45, order = 'hclust', type = 'upper')
```


### One-to-one relation between classes and covariates
```{r, cache=TRUE}
# Distribution of response classes with regard to each variable
transparentTheme(trans = .4)
featurePlot(x = train_df[,2:31], 
            y = train_df$diagnosis,
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")),
            plot = "density", pch = "|",
            auto.key = list(columns = 2))
```

# Model building, assessing performance, and variable importance

```{r, echo=F}
# only include mean
formula1 <- 
  diagnosis ~ radius_mean + texture_mean + perimeter_mean + area_mean + 
  smoothness_mean + compactness_mean + concavity_mean + concave_points_mean +
  symmetry_mean + fractal_dimension_mean
# include mean and worst measurements
formula2 <- 
  diagnosis ~ radius_mean + texture_mean + perimeter_mean + area_mean + 
  smoothness_mean + compactness_mean + concavity_mean + concave_points_mean +
  symmetry_mean + fractal_dimension_mean + radius_worst + texture_worst +
  perimeter_worst + area_worst + smoothness_worst + compactness_worst + concavity_worst +
  concave_points_worst + symmetry_worst + fractal_dimension_worst
# include all covariates
formula3 <- 
  diagnosis ~ radius_mean + texture_mean + perimeter_mean + area_mean + 
  smoothness_mean + compactness_mean + concavity_mean + concave_points_mean +
  symmetry_mean + fractal_dimension_mean + radius_worst + texture_worst +
  perimeter_worst + area_worst + smoothness_worst + compactness_worst + concavity_worst +
  concave_points_worst + symmetry_worst + fractal_dimension_worst + radius_se + texture_se +
  perimeter_se + area_se + smoothness_se + compactness_se + concavity_se + concave_points_se +
  symmetry_se + fractal_dimension_se
```

## Linear methods
### Logistic Regression
```{r, warning=FALSE}
glm.fit <- glm(diagnosis~., data=train_df, family="binomial")
# summary(glm.fit)

contrasts(train_df$diagnosis)

glm.pred.prob = predict(glm.fit, type = "response")
glm.pred = rep("B", length(glm.pred.prob))
glm.pred[glm.pred.prob > 0.5] = "M"
confusionMatrix(data = as.factor(glm.pred),
                reference = train_df$diagnosis,
                positive = "M")

glm.pred.prob.test = predict(glm.fit, type = "response", newdata = test_df)
roc.glm.test = roc(test_df$diagnosis, glm.pred.prob.test)
plot(roc.glm.test, legacy.axes = TRUE, print.auc = TRUE)
```

### Regularized Logistic Regression
```{r}
ctrl <- trainControl(method = "repeatedcv",
                    repeats = 5,
                    summaryFunction = twoClassSummary,
                    classProbs = TRUE)

glmnGrid <- expand.grid(.alpha = seq(0,1,length =6),
                       .lambda = exp(seq(-6,-2,length =20)))
```

```{r, eval=F}
set.seed(123123)
model.glmn <- train(x=train_df[,2:31],
                   y=train_df$diagnosis,
                   method ="glmnet",
                   tuneGrid =glmnGrid,
                   metric ="ROC",
                   trControl =ctrl)
saveRDS(model.glmn, "glmn_fit.rds")
```

```{r}
model.glmn = readRDS("glmn_fit.rds")
ggplot(model.glmn,xTrans = function(x)log(x), highlight = TRUE)
max(model.glmn$result$ROC)
model.glmn$bestTune
```

### LDA
```{r}
lda.fit <- lda(diagnosis~., data = train_df)
plot(lda.fit)

lda.pred.test = predict(lda.fit, newdata = test_df)
roc.lda = roc(test_df$diagnosis, lda.pred.test$posterior[,2],
levels = c("B", "M"))
plot(roc.lda, legacy.axes = TRUE, print.auc = TRUE)
```


## Non-linear methods

### QDA
```{r, eval=F}
set.seed(123123)
qda.fit <- qda(diagnosis~.,
               data = train_df)
ctrl <- trainControl(method = "repeatedcv",
                     repeats = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE) 
model.qda <- train(x = train_df[,-1],
                   y = train_df$diagnosis,
                   method = "qda",
                   metric = "ROC",
                   trControl = ctrl)
saveRDS(model.qda, "qda_fit.RDS")
```

```{r }
model.qda = readRDS("qda_fit.RDS")
# qda.pred <- predict(model.qda, newdata = test_df)
# head(qda.pred$posterior)

roc.qda <- roc(test_df$diagnosis, qda.pred$posterior[,2], 
               levels = c("B","M"))

plot(roc.qda, legacy.axes = TRUE, print.auc = TRUE,main="QDA ROC Plot")
```
**AUC Value for QDA is 0.990 as shown above.**

### KNN
```{r}
set.seed(123123)
model.knn <- train(x = train_df[,-1],
                   y = train_df$diagnosis,
                   method = "knn",
                   preProcess = c("center", "scale"), 
                   tuneGrid = data.frame(k = seq(1,50,by=1)),  
                   trControl = ctrl)
model.knn$bestTune
ggplot(model.knn)
pred_knn = predict.train(model.knn, newdata = test_df, type = 'prob')
roc_knn <- roc(test_df$diagnosis, pred_knn[,2],
               levels = c("B", "M"))
plot.roc(roc_knn, legacy.axes = TRUE, print.auc = TRUE,main="KNN ROC Plot")
```
**AUC Value for KNN is 0.989 as shown above.**

### Bayes
```{r,warning=F, eval=F}
set.seed(123123)
nbGrid <- expand.grid(usekernel = c(FALSE,TRUE),
                      fL = 1, 
                      adjust = seq(0,5,by = 1))
model.nb <- train(x = train_df[,-1],
                  y = train_df$diagnosis,
                  method = "nb",
                  tuneGrid = nbGrid,
                  metric = "ROC",
                  trControl = ctrl)
saveRDS(model.nb, "nb_fit.rds")
```

```{r}
model.nb = readRDS("nb_fit.rds")
plot(model.nb)
```

##### Compare QDA, NB and KNN   
```{r}
res <- resamples(list(QDA=model.qda,NB = model.nb, KNN = model.knn))
summary(res)
```

Now let's look at the test set performance.
```{r, warning=FALSE}
library(stats)
pred_knn = predict.train(model.knn, newdata = test_df, type = 'prob')[,2]
pred_qda = predict.train(model.qda, newdata = test_df, type = 'prob')[,2]
pred_nb = predict.train(model.nb, newdata = test_df, type = 'prob')[,2]

roc.nb <- roc(test_df$diagnosis, pred_nb)
roc.qda <- roc(test_df$diagnosis, pred_qda)
roc.knn <- roc(test_df$diagnosis, pred_knn)

auc <- c(roc.qda$auc[1], roc.nb$auc[1], roc.knn$auc[1])


plot(roc.qda, col = 1,legacy.axes=TRUE)
plot(roc.nb, col = 2,add=TRUE)
plot(roc.knn, col = 3,add=TRUE)
modelNames <- c("qda","nb","knn")
legend("bottomright", legend = paste0(modelNames, ": ", round(auc,3)),
       col = 1:6, lwd = 2)
```

## Support Vector Machine
### Linear Kernel
```{r}
##Linear Kernel
set.seed(123123)
svml.fit <- train(diagnosis~., 
                  data = train_df, 
                  method = "svmLinear2",
                  preProcess = c("center", "scale"),
                  tuneGrid = data.frame(cost = exp(seq(-3,-1,len=50))),
                  trControl = ctrl)
saveRDS(svml.fit, "svml_fit.rds")
```

```{r }
svml.fit = readRDS("svml_fit.rds")
svml.fit$bestTune
max(svml.fit$result$ROC)
ggplot(svml.fit, highlight = TRUE)
```

Linear Kernel Training Error Rate
```{r}
pred.svml.train <- predict(svml.fit)
mean(pred.svml.train != train_df$diagnosis)
```
**The training error rate for linear kernel is 0.0088.**

Linear Kernel Test Error Rate
```{r}
pred.svml.test <- predict(svml.fit, newdata = test_df)
mean(pred.svml.test != test_df$diagnosis)
```
**The testing error rate for linear kernel is 0.0265.**


### Radial Kernel  

Fit a support vector machine with a radial kernel to the training data. What are the training and test error rates?
```{r, eval=F}
svmr.grid <- expand.grid(C = seq(1,4,len=10),
                         sigma = exp(seq(-5,-2,len=10)))
set.seed(123123)
svmr.fit <- train(diagnosis~., 
                  data = train_df, 
                  method = "svmRadial",
                  preProcess = c("center", "scale"),
                  tuneGrid = svmr.grid,
                  trControl = ctrl)
saveRDS(svmr.fit, "svmr_fit.rds")
```

##### Radial Kernel Training Error Rate

```{r}
svmr.fit = readRDS("svmr_fit.rds")
svmr.fit$bestTune
ggplot(svmr.fit, highlight = TRUE)
pred.svmr.train <- predict(svmr.fit)
mean(pred.svmr.train != train_df$diagnosis)
```

**The training error rate for radial kernel is 0.011.**
### Raidal Kernel Test Error Rate
```{r}
pred.svmr.test <- predict(svmr.fit, newdata = test_df)
mean(pred.svmr.test != test_df$diagnosis)
```
**The testing error rati for radial kernel is 0.0177.**


##### Which approach seems to give a better result on this data?
```{r}
resamp <- resamples(list(svmr = svmr.fit, svml = svml.fit))
bwplot(resamp)
```
**Acccording to the plot, we can tell that radial kernal has higher accuracy and Kappa compared to linear kernal.**

```{r}
confusionMatrix(data = pred.svml.test, 
                reference = test_df$diagnosis)

confusionMatrix(data = pred.svmr.test, 
                reference = test_df$diagnosis)

```
**According to the confusion matrix,the radial kernel has higher sensitivity, specificity, PPV, NPV and Kappa compared to those of the linear kernel.**
**In conclusion, the radial kernel seems to give a better result on the data.**



## Classification Trees and Ensemble methods
### Classification Tree
```{r, eval=F}
ctrl <- trainControl(method = "repeatedcv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
# variable.names(train_df)

set.seed(123123)
rpart.fit <- train(formula3, train_df, 
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-20,-2, len = 50))),
                   trControl = ctrl,
                   metric = "ROC")
saveRDS(rpart.fit, 'rpart_fit.rds')
```

```{r }
rpart.fit = readRDS('rpart_fit.rds')
rpart.fit$bestTune
max(rpart.fit$result$ROC)
# Model tuning
ggplot(rpart.fit, highlight = TRUE)
# Plot of the final model
rpart.plot(rpart.fit$finalModel)
tree_pruned = prune(tree_rpart, cp = one_se)
rpart.plot(tree_pruned)
```

The plot of the final model is shown above.

```{r}
test_df$probM = predict(rpart.fit$finalModel, newdata = test_df, type = "prob")[,1]
test_df$pred = if_else(test_df$probM > 0.5, 'M', 'B')

# Classification error rate
1 - mean(test_df$pred == test_df$diagnosis)
max(rpart.fit$result[,"ROC"])
```

### Random forests
```{r, eval = F}
rf.grid <- expand.grid(mtry = seq(1,10, by=1),
                       splitrule = "gini",
                       min.node.size = seq(1,31, by=2))
set.seed(123123)
rf.fit <- train(formula1, train_df,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl)
```

```{r, echo=F}
# saveRDS(rf.fit, 'rf_fit.rds')
rf.fit = readRDS('rf_fit.rds')
```

```{r }
rf.fit$bestTune
max(rf.fit$results[,"ROC"])
ggplot(rf.fit, highlight = TRUE)

rf.pred <- predict(rf.fit, newdata = test_df, type = "prob")[,1]
pred_rf = if_else(test_df$probM3 > 0.5, 'M', 'B')
# Classification error rate
1 - mean(pred_rf == test_df$diagnosis)
```

The test classification error rate for random forest is `r 1 - mean(pred_rf == test_df$Purchase)`.

```{r, eval = F}
rf.grid <- expand.grid(mtry = seq(1,10, by=1),
                       splitrule = "gini",
                       min.node.size = seq(1,31, by=2))
set.seed(123123)
rf.fit2 <- train(formula2, train_df,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl)
max(rf.fit2$result$ROC)
```

```{r, eval = F}
rf.grid <- expand.grid(mtry = seq(1,10, by=1),
                       splitrule = "gini",
                       min.node.size = seq(1,31, by=2))
set.seed(123123)
rf.fit3 <- train(formula3, train_df,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl)
max(rf.fit3$result$ROC)
```

### boosting (Adaboosting and binomial loss function)

#### Binomial loss
```{r, eval=F}
gbmB.grid <- expand.grid(n.trees = c(1000,2000,3000,4000),
                        interaction.depth = 1:8,
                        shrinkage = c(0.003,0.004,0.005,0.006,0.007),
                        n.minobsinnode = 1)
set.seed(1)
# Binomial loss function
gbmB.fit <- train(formula1, train_df, 
                 tuneGrid = gbmB.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "bernoulli",
                 metric = "ROC",
                 verbose = FALSE)

gbmB.fit$bestTune
max(gbmB.fit$results[,"ROC"])
# gbmB.fit$results %>% as_tibble %>% filter(ROC == max(ROC))

gbmB.fit2 <- train(formula2, train_df, 
                 tuneGrid = gbmB.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "bernoulli",
                 metric = "ROC",
                 verbose = FALSE)
gbmB.fit2$bestTune
max(gbmB.fit2$results[,"ROC"])

gbmB.fit3 <- train(formula3, train_df, 
                 tuneGrid = gbmB.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "bernoulli",
                 metric = "ROC",
                 verbose = FALSE)
gbmB.fit3$bestTune
max(gbmB.fit3$results[,"ROC"])
```

```{r, echo=F,include=F}
# saveRDS(gbmB.fit, 'gbmB_fit.rds')
gbmB.fit = readRDS('gbmB_fit.rds')
```

```{r}
ggplot(gbmB.fit, highlight = TRUE)
gbmB.pred <- predict(gbmB.fit, newdata = test_df, type = "prob")[,1]
class_gbmB = if_else(test_df$probM3 > 0.5, 'M', 'B')
# Classification error rate
1 - mean(class_gbmB == test_df$diagnosis)
```


#### AdaBoost
```{r, eval=F}
gbmA.grid <- expand.grid(n.trees = c(6000,8000,10000,12000,14000),
                        interaction.depth = seq(1, 10, by=2),
                        shrinkage = seq(0.004, 0.010, by=0.002),
                        n.minobsinnode = 1)
set.seed(123123)
# Adaboost loss function
gbmA.fit <- train(formula3, train_df,
                 tuneGrid = gbmA.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "adaboost",
                 metric = "ROC",
                 verbose = FALSE)
```

```{r, echo=F}
# saveRDS(gbmA.fit, 'gbmA_fit.rds')
gbmA.fit = readRDS('hw4_gbmA_fit.rds')
```

```{r}
gbmA.fit$bestTune
max(gbmA.fit$results[,"ROC"])
ggplot(gbmA.fit, highlight = TRUE)
gbmA.pred <- predict(gbmA.fit, newdata = test_df, type = "prob")[,1]
class_gbmA = if_else(gbmA.pred > 0.5, 'CH', 'MM')
# Classification error rate
1 - mean(class_gbmA == test_df$Purchase)
```

# Variable importance, Visualization, and interpretation (L7_2.rmd): PDP, ICE, etc.

# Model comparison

# Final model interpretation


